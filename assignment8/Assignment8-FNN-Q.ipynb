{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8: Neural Networks\n",
    "\n",
    "Only use the already imported library `numpy` and the Python standard library. For the evaluation you may also use scikit-learn (`sklearn`) and `matplotlib`. Make sure that the dataset `airfoil_self_noise.csv` is in the same directory as the notebook.\n",
    "\n",
    "List your team members (name and immatriculation number) and indicate whether you are a B.Sc. Data Science or other group in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> *Write*\n",
    "* *names* Srinivas Kumar Ramdas, Benedikt Riegel, Fozan Gill \n",
    "* *matr. nr.* 3513675, 3568633, 3437081\n",
    "* *study program* Computer Science, Computer Science, Computer Science\n",
    "* *B.Sc./M.Sc.* M.Sc., M.Sc, M.Sc\n",
    "\n",
    "*of all assignment group participants here. (double klick here to edit)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    data = np.genfromtxt(path)\n",
    "    X, y = data[:, :5], data[:, 5]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = load_dataset('airfoil_self_noise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_max = np.max(X_train, axis=0)\n",
    "X_train_min = np.min(X_test, axis=0)\n",
    "\n",
    "y_train_max = np.max(y_train, axis=0)\n",
    "y_train_min = np.min(y_train, axis=0)\n",
    "\n",
    "def normalize_dataset():\n",
    "    global X_train, X_test, y_train\n",
    "    X_train = (X_train - X_train_min)/(X_train_max - X_train_min)\n",
    "    X_test = (X_test - X_train_min)/(X_train_max - X_train_min)\n",
    "    \n",
    "    y_train = (y_train - y_train_min)/(y_train_max - y_train_min)\n",
    "\n",
    "    return\n",
    "\n",
    "#normalize_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Feedforward Neural Network: Programming (40 Points)\n",
    "\n",
    "In this task, you will implement a feedforward neural network for regression. The hyperparameters of the model are:\n",
    "- `input_dim`: The dimension of the input vector.\n",
    "- `output_dim`: The dimension of the output vector.\n",
    "- `width`: The dimension of each hidden layer.\n",
    "- `depth`: The number of hidden layers. For B.Sc. Data Science students, this parameter is constant with a value of 1.\n",
    "- `learning_rate`: The learning rate for gradient descent.\n",
    "- `epochs`: The number of epochs/iterations performed during training.\n",
    "\n",
    "B.Sc. Data Science only have to implement for a single hidden layer, i.e. `depth = 1`. All other students have to implement the network for any `depth >= 1`.\n",
    "\n",
    "The activation function for each hidden layer is ReLU (g(x) = max(0, x)). The output layer uses the identity as activation, since our objective is regression.\n",
    "\n",
    "You have to implement the `FeedforwardNeuralNetworkRegressor`.\n",
    "\n",
    "The `__init__` method initializes the network.\n",
    "Initialize each weight and bias randomly with a standard Gaussian distribution using the numpy function `numpy.random.normal` with default parameters.\n",
    "\n",
    "The `fit` method trains the network.\n",
    "Use backpropagation with gradient descent similar to Task 2.\n",
    "Use the whole training data set for each training epoch.\n",
    "Use the mean squared error as loss function.\n",
    "\n",
    "The `predict` method computes the forward-pass of the network.\n",
    "\n",
    "Evaluate your classifier on the test data with the mean squared error and compare your results to your linear regression model from assignment 3. Try out different hyper-parameters and compare the results. You may want to normalize your input and output data for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkClassifier(object):\n",
    "    def __init__(self, input_dim, output_dim, width, depth, learning_rate, epochs):\n",
    "        # Add your code, such as initialization of weights here.\n",
    "        \n",
    "        self.weights_list = []\n",
    "        self.biases_list = []\n",
    "        self.memory = {}\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        prev_dim = input_dim\n",
    "        curr_dim = width\n",
    "        \n",
    "        np.random.seed(27)\n",
    "        \n",
    "        for layer in range(depth):\n",
    "            self.weights_list.append(np.random.normal(size = (curr_dim, prev_dim)))\n",
    "            self.biases_list.append(np.random.normal(size = (curr_dim, 1)))\n",
    "            \n",
    "            prev_dim = curr_dim\n",
    "            curr_dim = width\n",
    "        \n",
    "        #Appending the final output layer\n",
    "        self.weights_list.append(np.random.normal(size = (output_dim, prev_dim)))\n",
    "        self.biases_list.append(np.random.normal(size = (output_dim, 1)))\n",
    "        \n",
    "    \n",
    "    def relu(self, z):\n",
    "        return z*(z > 0)\n",
    "    \n",
    "    def relu_backward(self, dA, z):\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[z <= 0] = True\n",
    "        return dZ\n",
    "    \n",
    "    def identity_backward(self, dA, z):\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        return dZ\n",
    "        \n",
    "    def identity(self, z):\n",
    "        return z\n",
    "    \n",
    "    def single_layer_backward_propagation(self, dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            backward_activation_func = self.relu_backward\n",
    "        else:\n",
    "            backward_activation_func = self.identity_backward\n",
    "        \n",
    "        dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "        \n",
    "        # Derivative of matrix W\n",
    "        dW_curr = np.dot(dZ_curr, A_prev.T)/m \n",
    "        \n",
    "        #Derivative of the vector b\n",
    "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True)/m\n",
    "        \n",
    "        #Derivative of matrix A_prev\n",
    "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "        \n",
    "        return dA_prev, dW_curr, db_curr\n",
    "    \n",
    "    def full_backward_propagation(self, Y_hat, Y):\n",
    "        grad_values = {}\n",
    "        \n",
    "        #Number of values\n",
    "        m = Y.shape[0]\n",
    "        \n",
    "        #Initiation of gradient descent algorithm\n",
    "        dA_prev = (2/m)*(Y_hat - Y)\n",
    "        \n",
    "        for idx in range(len(self.weights_list) - 1 , -1, -1):\n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            activation = \"relu\"\n",
    "            if (layer_idx == len(self.weights_list)):\n",
    "                activation = \"identity\"\n",
    "            \n",
    "            dA_curr = dA_prev\n",
    "            \n",
    "            A_prev = self.memory[\"A\" + str(idx)]\n",
    "            z_curr = self.memory[\"Z\" + str(layer_idx)]\n",
    "            \n",
    "            W_curr = self.weights_list[idx]\n",
    "            b_curr = self.biases_list[idx]\n",
    "            \n",
    "            dA_prev, dW_curr, db_curr = self.single_layer_backward_propagation(dA_curr, W_curr, b_curr, z_curr, A_prev, activation)\n",
    "            grad_values[\"dW\" + str(idx)] = dW_curr\n",
    "            grad_values[\"db\" + str(idx)] = db_curr\n",
    "            \n",
    "        return grad_values\n",
    "    \n",
    "    def update(self, grad_values):\n",
    "        for idx in range(len(self.weights_list) - 1, -1, -1):\n",
    "            self.weights_list[idx] -= self.learning_rate*grad_values[\"dW\" + str(idx)]\n",
    "            self.biases_list[idx] -= self.learning_rate*grad_values[\"db\" + str(idx)]\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        z_prev = X\n",
    "        A_curr = None\n",
    "        \n",
    "        A_curr = X \n",
    "        z_curr = None\n",
    "        \n",
    "        for idx in range(len(self.weights_list) - 1 ):\n",
    "            \n",
    "            A_prev = A_curr \n",
    "            z_curr = np.dot(self.weights_list[idx], A_prev) + self.biases_list[idx]\n",
    "            A_curr = self.relu(z_curr)\n",
    "            \n",
    "            #Storing in memory\n",
    "            self.memory[\"A\" + str(idx)] =  A_prev\n",
    "            self.memory[\"Z\" + str(idx+1)] = z_curr\n",
    "                    \n",
    "        idx = len(self.weights_list) - 1\n",
    "        A_prev = A_curr\n",
    "        z_curr = np.dot(self.weights_list[idx], A_prev) + self.biases_list[idx]\n",
    "        A_curr = self.identity(z_curr)\n",
    "        \n",
    "        #Storing in memory\n",
    "        self.memory[\"A\" + str(idx)] =  A_prev\n",
    "        self.memory[\"Z\" + str(idx+1)] = z_curr\n",
    "        \n",
    "        return A_curr\n",
    "    \n",
    "    def compute_rmse(self, y, y_hat):\n",
    "        return (np.mean((y - y_hat)**2))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Implement your training here.\n",
    "        \n",
    "        epochs = []\n",
    "        errors = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            y_hat = self.forward_pass(X)\n",
    "\n",
    "            #print(\"The error is : \" + str(self.compute_rmse(y, y_hat)))\n",
    "            grad_values = self.full_backward_propagation(y_hat, y)\n",
    "            self.update(grad_values)\n",
    "            \n",
    "            epochs.append(epoch)\n",
    "            errors.append(self.compute_rmse(y, y_hat))\n",
    "            \n",
    "            if epoch%100 == 0:\n",
    "                print(\"After epoch %s error is %s \" % (epoch, self.compute_rmse(y, y_hat), \n",
    "                                                                ))\n",
    "        \n",
    "        self.plot(epochs, errors)\n",
    "        return \n",
    "    \n",
    "    def plot(self, epochs, errors):\n",
    "        plt.plot(epochs, errors)\n",
    "        plt.title(\"Error vs Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Implement your prediction here.\n",
    "        y_hat = self.forward_pass(X)\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 0 error is 12826704.374085514 \n",
      "After epoch 100 error is 15063.305860570163 \n",
      "After epoch 200 error is 14503.062675718034 \n",
      "After epoch 300 error is 13963.723565717253 \n",
      "After epoch 400 error is 13444.508547236997 \n",
      "After epoch 500 error is 12944.666740075894 \n",
      "After epoch 600 error is 12463.475281251442 \n",
      "After epoch 700 error is 12000.238279607494 \n",
      "After epoch 800 error is 11554.285809427918 \n",
      "After epoch 900 error is 11124.972941601089 \n",
      "After epoch 1000 error is 10711.678810934034 \n",
      "After epoch 1100 error is 10313.805718267413 \n",
      "After epoch 1200 error is 9930.778266092844 \n",
      "After epoch 1300 error is 9562.042526422536 \n",
      "After epoch 1400 error is 9207.06523970774 \n",
      "After epoch 1500 error is 8865.333043647594 \n",
      "After epoch 1600 error is 8536.351730773007 \n",
      "After epoch 1700 error is 8219.64553373198 \n",
      "After epoch 1800 error is 7914.75643724271 \n",
      "After epoch 1900 error is 7621.243515719417 \n",
      "After epoch 2000 error is 7338.682295613026 \n",
      "After epoch 2100 error is 7066.66414154456 \n",
      "After epoch 2200 error is 6804.795665343347 \n",
      "After epoch 2300 error is 6552.698157135543 \n",
      "After epoch 2400 error is 6310.007037660176 \n",
      "After epoch 2500 error is 6076.371331020589 \n",
      "After epoch 2600 error is 5851.45315710892 \n",
      "After epoch 2700 error is 5634.927242969446 \n",
      "After epoch 2800 error is 5426.480452394174 \n",
      "After epoch 2900 error is 5225.811333070472 \n",
      "After epoch 3000 error is 5032.629680625646 \n",
      "After epoch 3100 error is 4846.656118938195 \n",
      "After epoch 3200 error is 4667.621696108644 \n",
      "After epoch 3300 error is 4495.267495505786 \n",
      "After epoch 3400 error is 4329.344261325653 \n",
      "After epoch 3500 error is 4169.612038121929 \n",
      "After epoch 3600 error is 4015.8398237863253 \n",
      "After epoch 3700 error is 3867.8052354771376 \n",
      "After epoch 3800 error is 3725.2941880128396 \n",
      "After epoch 3900 error is 3588.100584265636 \n",
      "After epoch 4000 error is 3456.026017107185 \n",
      "After epoch 4100 error is 3328.8794824754837 \n",
      "After epoch 4200 error is 3206.4771031479386 \n",
      "After epoch 4300 error is 3088.6418628211445 \n",
      "After epoch 4400 error is 2975.203350112845 \n",
      "After epoch 4500 error is 2865.997512115819 \n",
      "After epoch 4600 error is 2760.8664171472037 \n",
      "After epoch 4700 error is 2659.6580263503633 \n",
      "After epoch 4800 error is 2562.2259738187645 \n",
      "After epoch 4900 error is 2468.4293549240533 \n",
      "After epoch 5000 error is 2378.1325225420364 \n",
      "After epoch 5100 error is 2291.204890882127 \n",
      "After epoch 5200 error is 2207.5207466362576 \n",
      "After epoch 5300 error is 2126.9590671743954 \n",
      "After epoch 5400 error is 2049.4033455236295 \n",
      "After epoch 5500 error is 1974.7414218776778 \n",
      "After epoch 5600 error is 1902.865321393238 \n",
      "After epoch 5700 error is 1833.6710980385508 \n",
      "After epoch 5800 error is 1767.0586842683376 \n",
      "After epoch 5900 error is 1702.9317463077914 \n",
      "After epoch 6000 error is 1641.197544836255 \n",
      "After epoch 6100 error is 1581.7668008691362 \n",
      "After epoch 6200 error is 1524.553566644129 \n",
      "After epoch 6300 error is 1469.4751013249577 \n",
      "After epoch 6400 error is 1416.451751342947 \n",
      "After epoch 6500 error is 1365.406835203345 \n",
      "After epoch 6600 error is 1316.266532589832 \n",
      "After epoch 6700 error is 1268.959777606742 \n",
      "After epoch 6800 error is 1223.4181560047925 \n",
      "After epoch 6900 error is 1179.5758062415046 \n",
      "After epoch 7000 error is 1137.3693242333102 \n",
      "After epoch 7100 error is 1096.737671661631 \n",
      "After epoch 7200 error is 1057.622087700254 \n",
      "After epoch 7300 error is 1019.966004036409 \n",
      "After epoch 7400 error is 983.714963062602 \n",
      "After epoch 7500 error is 948.8165391209042 \n",
      "After epoch 7600 error is 915.2202626858564 \n",
      "After epoch 7700 error is 882.8775473762522 \n",
      "After epoch 7800 error is 851.7416196903205 \n",
      "After epoch 7900 error is 821.7674513626662 \n",
      "After epoch 8000 error is 792.9116942451233 \n",
      "After epoch 8100 error is 765.1326176173925 \n",
      "After epoch 8200 error is 738.3900478367339 \n",
      "After epoch 8300 error is 712.6453102395428 \n",
      "After epoch 8400 error is 687.8611732106606 \n",
      "After epoch 8500 error is 664.0017943396392 \n",
      "After epoch 8600 error is 641.0326685860362 \n",
      "After epoch 8700 error is 618.9205783787868 \n",
      "After epoch 8800 error is 597.6335455775163 \n",
      "After epoch 8900 error is 577.1407852262689 \n",
      "After epoch 9000 error is 557.4126610328171 \n",
      "After epoch 9100 error is 538.420642509126 \n",
      "After epoch 9200 error is 520.1372637110596 \n",
      "After epoch 9300 error is 502.5360835175722 \n",
      "After epoch 9400 error is 485.59164739199355 \n",
      "After epoch 9500 error is 469.2794505700924 \n",
      "After epoch 9600 error is 453.57590262170584 \n",
      "After epoch 9700 error is 438.45829333463894 \n",
      "After epoch 9800 error is 423.9047598715261 \n",
      "After epoch 9900 error is 409.8942551521635 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "399.5961322987295"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement your training and evaluation here.\n",
    "X_train_T = X_train.T\n",
    "X_test_T = X_test.T\n",
    "\n",
    "input_dim = X_train_T.shape[0]\n",
    "output_dim = 1\n",
    "width = 2\n",
    "depth = 2\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "ffnn = FeedforwardNeuralNetworkClassifier(input_dim, output_dim, width, depth, learning_rate, epochs)\n",
    "y_hat = ffnn.fit(X_train_T, y_train)\n",
    "#print(ffnn.memory.keys())\n",
    "#grad_values = ffnn.full_backward_propagation(y_hat, y_train)\n",
    "#ffnn.update(grad_values)\n",
    "y_hat_test = ffnn.predict(X_test_T)\n",
    "\n",
    "# Rescaling the test predictions\n",
    "#y_hat_test = y_hat_test*(y_train_max - y_train_min) + y_train_min\n",
    "\n",
    "#print(y_hat_test)\n",
    "ffnn.compute_rmse(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}