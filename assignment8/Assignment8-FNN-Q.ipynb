{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8: Neural Networks\n",
    "\n",
    "Only use the already imported library `numpy` and the Python standard library. For the evaluation you may also use scikit-learn (`sklearn`) and `matplotlib`. Make sure that the dataset `airfoil_self_noise.csv` is in the same directory as the notebook.\n",
    "\n",
    "List your team members (name and immatriculation number) and indicate whether you are a B.Sc. Data Science or other group in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> *Write*\n",
    "* *names* \n",
    "* *matr. nr.* \n",
    "* *study program*\n",
    "* *B.Sc./M.Sc.*\n",
    "\n",
    "*of all assignment group participants here. (double klick here to edit)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    data = np.genfromtxt(path)\n",
    "    X, y = data[:, :5], data[:, 5]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = load_dataset('airfoil_self_noise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.13\n",
      "140.987\n"
     ]
    }
   ],
   "source": [
    "X_train_max = np.max(X_train, axis=0)\n",
    "X_train_min = np.min(X_test, axis=0)\n",
    "\n",
    "y_train_max = np.max(y_train, axis=0)\n",
    "y_train_min = np.min(y_train, axis=0)\n",
    "print(y_train_min)\n",
    "print(y_train_max)\n",
    "\n",
    "def normalize_dataset():\n",
    "    global X_train, X_test, y_train\n",
    "    X_train = (X_train - X_train_min)/(X_train_max - X_train_min)\n",
    "    X_test = (X_test - X_train_min)/(X_train_max - X_train_min)\n",
    "    \n",
    "    y_train = (y_train - y_train_min)/(y_train_max - y_train_min)\n",
    "\n",
    "    return\n",
    "\n",
    "#normalize_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Feedforward Neural Network: Programming (40 Points)\n",
    "\n",
    "In this task, you will implement a feedforward neural network for regression. The hyperparameters of the model are:\n",
    "- `input_dim`: The dimension of the input vector.\n",
    "- `output_dim`: The dimension of the output vector.\n",
    "- `width`: The dimension of each hidden layer.\n",
    "- `depth`: The number of hidden layers. For B.Sc. Data Science students, this parameter is constant with a value of 1.\n",
    "- `learning_rate`: The learning rate for gradient descent.\n",
    "- `epochs`: The number of epochs/iterations performed during training.\n",
    "\n",
    "B.Sc. Data Science only have to implement for a single hidden layer, i.e. `depth = 1`. All other students have to implement the network for any `depth >= 1`.\n",
    "\n",
    "The activation function for each hidden layer is ReLU (g(x) = max(0, x)). The output layer uses the identity as activation, since our objective is regression.\n",
    "\n",
    "You have to implement the `FeedforwardNeuralNetworkRegressor`.\n",
    "\n",
    "The `__init__` method initializes the network.\n",
    "Initialize each weight and bias randomly with a standard Gaussian distribution using the numpy function `numpy.random.normal` with default parameters.\n",
    "\n",
    "The `fit` method trains the network.\n",
    "Use backpropagation with gradient descent similar to Task 2.\n",
    "Use the whole training data set for each training epoch.\n",
    "Use the mean squared error as loss function.\n",
    "\n",
    "The `predict` method computes the forward-pass of the network.\n",
    "\n",
    "Evaluate your classifier on the test data with the mean squared error and compare your results to your linear regression model from assignment 3. Try out different hyper-parameters and compare the results. You may want to normalize your input and output data for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkClassifier(object):\n",
    "    def __init__(self, input_dim, output_dim, width, depth, learning_rate, epochs):\n",
    "        # Add your code, such as initialization of weights here.\n",
    "        \n",
    "        self.weights_list = []\n",
    "        self.biases_list = []\n",
    "        self.memory = {}\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        prev_dim = input_dim\n",
    "        curr_dim = width\n",
    "        \n",
    "        np.random.seed(27)\n",
    "        \n",
    "        for layer in range(depth):\n",
    "            self.weights_list.append(np.random.normal(size = (curr_dim, prev_dim)))\n",
    "            self.biases_list.append(np.random.normal(size = (curr_dim, 1)))\n",
    "            \n",
    "            prev_dim = curr_dim\n",
    "            curr_dim = width\n",
    "        \n",
    "        #Appending the final output layer\n",
    "        self.weights_list.append(np.random.normal(size = (output_dim, prev_dim)))\n",
    "        self.biases_list.append(np.random.normal(size = (output_dim, 1)))\n",
    "        \n",
    "    \n",
    "    def relu(self, z):\n",
    "        return z*(z > 0)\n",
    "    \n",
    "    def relu_backward(self, dA, z):\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[z <= 0] = True\n",
    "        return dZ\n",
    "    \n",
    "    def identity_backward(self, dA, z):\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        return dZ\n",
    "        \n",
    "    def identity(self, z):\n",
    "        return z\n",
    "    \n",
    "    def single_layer_backward_propagation(self, dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            backward_activation_func = self.relu_backward\n",
    "        else:\n",
    "            backward_activation_func = self.identity_backward\n",
    "        \n",
    "        dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "        \n",
    "        # Derivative of matrix W\n",
    "        dW_curr = np.dot(dZ_curr, A_prev.T)/m \n",
    "        \n",
    "        #Derivative of the vector b\n",
    "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True)/m\n",
    "        \n",
    "        #Derivative of matrix A_prev\n",
    "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "        \n",
    "        return dA_prev, dW_curr, db_curr\n",
    "    \n",
    "    def full_backward_propagation(self, Y_hat, Y):\n",
    "        grad_values = {}\n",
    "        \n",
    "        #Number of values\n",
    "        m = Y.shape[0]\n",
    "        \n",
    "        #Initiation of gradient descent algorithm\n",
    "        dA_prev = (2/m)*(Y_hat - Y)\n",
    "        \n",
    "        for idx in range(len(self.weights_list) - 1 , -1, -1):\n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            activation = \"relu\"\n",
    "            if (layer_idx == len(self.weights_list)):\n",
    "                activation = \"identity\"\n",
    "            \n",
    "            dA_curr = dA_prev\n",
    "            \n",
    "            A_prev = self.memory[\"A\" + str(idx)]\n",
    "            z_curr = self.memory[\"Z\" + str(layer_idx)]\n",
    "            \n",
    "            W_curr = self.weights_list[idx]\n",
    "            b_curr = self.biases_list[idx]\n",
    "            \n",
    "            dA_prev, dW_curr, db_curr = self.single_layer_backward_propagation(dA_curr, W_curr, b_curr, z_curr, A_prev, activation)\n",
    "            grad_values[\"dW\" + str(idx)] = dW_curr\n",
    "            grad_values[\"db\" + str(idx)] = db_curr\n",
    "            \n",
    "        return grad_values\n",
    "    \n",
    "    def update(self, grad_values):\n",
    "        for idx in range(len(self.weights_list) - 1, -1, -1):\n",
    "            self.weights_list[idx] -= self.learning_rate*grad_values[\"dW\" + str(idx)]\n",
    "            self.biases_list[idx] -= self.learning_rate*grad_values[\"db\" + str(idx)]\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        z_prev = X\n",
    "        A_curr = None\n",
    "        \n",
    "        A_curr = X \n",
    "        z_curr = None\n",
    "        \n",
    "        for idx in range(len(self.weights_list) - 1 ):\n",
    "            \n",
    "            A_prev = A_curr \n",
    "            z_curr = np.dot(self.weights_list[idx], A_prev) + self.biases_list[idx]\n",
    "            A_curr = self.relu(z_curr)\n",
    "            \n",
    "            #Storing in memory\n",
    "            self.memory[\"A\" + str(idx)] =  A_prev\n",
    "            self.memory[\"Z\" + str(idx+1)] = z_curr\n",
    "                    \n",
    "        idx = len(self.weights_list) - 1\n",
    "        A_prev = A_curr\n",
    "        z_curr = np.dot(self.weights_list[idx], A_prev) + self.biases_list[idx]\n",
    "        A_curr = self.identity(z_curr)\n",
    "        \n",
    "        #Storing in memory\n",
    "        self.memory[\"A\" + str(idx)] =  A_prev\n",
    "        self.memory[\"Z\" + str(idx+1)] = z_curr\n",
    "        \n",
    "        return A_curr\n",
    "    \n",
    "    def compute_mse(self, y, y_hat):\n",
    "        return (np.mean((y - y_hat)**2))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Implement your training here.\n",
    "        \n",
    "        epochs = []\n",
    "        errors = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            y_hat = self.forward_pass(X)\n",
    "\n",
    "            #print(\"The error is : \" + str(self.compute_mse(y, y_hat)))\n",
    "            grad_values = self.full_backward_propagation(y_hat, y)\n",
    "            self.update(grad_values)\n",
    "            \n",
    "            epochs.append(epoch)\n",
    "            errors.append(self.compute_mse(y, y_hat))\n",
    "            \n",
    "            if epoch%100 == 0:\n",
    "                print(\"After epoch %s error is %s \" % (epoch, self.compute_mse(y, y_hat), \n",
    "                                                                ))\n",
    "        \n",
    "        self.plot(epochs, errors)\n",
    "        return \n",
    "    \n",
    "    def plot(self, epochs, errors):\n",
    "        plt.plot(epochs, errors)\n",
    "        plt.title(\"Error vs Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Implement your prediction here.\n",
    "        y_hat = self.forward_pass(X)\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 0 error is 125809438.5631427 \n",
      "After epoch 100 error is 11199.815695491994 \n",
      "After epoch 200 error is 8592.405123788662 \n",
      "After epoch 300 error is 6594.656905966455 \n",
      "After epoch 400 error is 5064.0203483319565 \n",
      "After epoch 500 error is 3891.275829945559 \n",
      "After epoch 600 error is 2992.741361690843 \n",
      "After epoch 700 error is 2304.3014005707096 \n",
      "After epoch 800 error is 1776.8318411806554 \n",
      "After epoch 900 error is 1372.6947314892652 \n",
      "After epoch 1000 error is 1063.0525909673154 \n",
      "After epoch 1100 error is 825.8106923942483 \n",
      "After epoch 1200 error is 644.0404774497403 \n",
      "After epoch 1300 error is 504.7716078289022 \n",
      "After epoch 1400 error is 398.06645786036717 \n",
      "After epoch 1500 error is 316.3110084135107 \n",
      "After epoch 1600 error is 253.67154334337522 \n",
      "After epoch 1700 error is 205.67838067569147 \n",
      "After epoch 1800 error is 168.90693538528544 \n",
      "After epoch 1900 error is 140.73335578379428 \n",
      "After epoch 2000 error is 119.14729678358717 \n",
      "After epoch 2100 error is 102.60847034579305 \n",
      "After epoch 2200 error is 89.93673717130147 \n",
      "After epoch 2300 error is 80.22789705199368 \n",
      "After epoch 2400 error is 72.78916904499837 \n",
      "After epoch 2500 error is 67.0897576135515 \n",
      "After epoch 2600 error is 62.722977347577064 \n",
      "After epoch 2700 error is 59.37723364743072 \n",
      "After epoch 2800 error is 56.8137886776224 \n",
      "After epoch 2900 error is 54.84972606489252 \n",
      "After epoch 3000 error is 53.34489877499259 \n",
      "After epoch 3100 error is 52.19192882497688 \n",
      "After epoch 3200 error is 51.30854525361154 \n",
      "After epoch 3300 error is 50.631713620534526 \n",
      "After epoch 3400 error is 50.11313814054728 \n",
      "After epoch 3500 error is 49.71581550475147 \n",
      "After epoch 3600 error is 49.41139448407467 \n",
      "After epoch 3700 error is 49.17815290788215 \n",
      "After epoch 3800 error is 48.99944766359987 \n",
      "After epoch 3900 error is 48.86252711600573 \n",
      "After epoch 4000 error is 48.75762120555648 \n",
      "After epoch 4100 error is 48.67724429909108 \n",
      "After epoch 4200 error is 48.615661047343934 \n",
      "After epoch 4300 error is 48.568477135166546 \n",
      "After epoch 4400 error is 48.532325722157324 \n",
      "After epoch 4500 error is 48.504627199456394 \n",
      "After epoch 4600 error is 48.48340511998697 \n",
      "After epoch 4700 error is 48.46714516771915 \n",
      "After epoch 4800 error is 48.454687102611395 \n",
      "After epoch 4900 error is 48.445141970886986 \n",
      "After epoch 5000 error is 48.4378286731278 \n",
      "After epoch 5100 error is 48.43222536395841 \n",
      "After epoch 5200 error is 48.42793221541152 \n",
      "After epoch 5300 error is 48.42464288692928 \n",
      "After epoch 5400 error is 48.42212266622275 \n",
      "After epoch 5500 error is 48.42019172121561 \n",
      "After epoch 5600 error is 48.41871226800308 \n",
      "After epoch 5700 error is 48.417578739186865 \n",
      "After epoch 5800 error is 48.41671025104118 \n",
      "After epoch 5900 error is 48.41604483199899 \n",
      "After epoch 6000 error is 48.415535000628324 \n",
      "After epoch 6100 error is 48.415144377562214 \n",
      "After epoch 6200 error is 48.41484508962397 \n",
      "After epoch 6300 error is 48.41461578091757 \n",
      "After epoch 6400 error is 48.41444008896306 \n",
      "After epoch 6500 error is 48.41430547714059 \n",
      "After epoch 6600 error is 48.41420234013127 \n",
      "After epoch 6700 error is 48.41412331852309 \n",
      "After epoch 6800 error is 48.41406277367502 \n",
      "After epoch 6900 error is 48.41401638536792 \n",
      "After epoch 7000 error is 48.41398084353235 \n",
      "After epoch 7100 error is 48.41395361205623 \n",
      "After epoch 7200 error is 48.41393274781895 \n",
      "After epoch 7300 error is 48.41391676203853 \n",
      "After epoch 7400 error is 48.41390451403867 \n",
      "After epoch 7500 error is 48.41389512985492 \n",
      "After epoch 7600 error is 48.41388793987243 \n",
      "After epoch 7700 error is 48.41388243104507 \n",
      "After epoch 7800 error is 48.413878210286676 \n",
      "After epoch 7900 error is 48.41387497642215 \n",
      "After epoch 8000 error is 48.413872498696875 \n",
      "After epoch 8100 error is 48.41387060031106 \n",
      "After epoch 8200 error is 48.413869145804064 \n",
      "After epoch 8300 error is 48.41386803138857 \n",
      "After epoch 8400 error is 48.41386717754469 \n",
      "After epoch 8500 error is 48.413866523345796 \n",
      "After epoch 8600 error is 48.413866022111094 \n",
      "After epoch 8700 error is 48.413865638074626 \n",
      "After epoch 8800 error is 48.41386534383322 \n",
      "After epoch 8900 error is 48.413865118391065 \n",
      "After epoch 9000 error is 48.4138649456616 \n",
      "After epoch 9100 error is 48.41386481331956 \n",
      "After epoch 9200 error is 48.41386471192164 \n",
      "After epoch 9300 error is 48.41386463423247 \n",
      "After epoch 9400 error is 48.41386457470852 \n",
      "After epoch 9500 error is 48.413864529102405 \n",
      "After epoch 9600 error is 48.41386449415987 \n",
      "After epoch 9700 error is 48.413864467387555 \n",
      "After epoch 9800 error is 48.41386444687513 \n",
      "After epoch 9900 error is 48.413864431158906 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing MSE is : 45.575508010298805\n"
     ]
    }
   ],
   "source": [
    "# Implement your training and evaluation here.\n",
    "X_train_T = X_train.T\n",
    "X_test_T = X_test.T\n",
    "\n",
    "input_dim = X_train_T.shape[0]\n",
    "output_dim = 1\n",
    "width = 3\n",
    "depth = 3\n",
    "learning_rate = 0.7\n",
    "epochs = 10000\n",
    "\n",
    "ffnn = FeedforwardNeuralNetworkClassifier(input_dim, output_dim, width, depth, learning_rate, epochs)\n",
    "y_hat = ffnn.fit(X_train_T, y_train)\n",
    "#print(ffnn.memory.keys())\n",
    "#grad_values = ffnn.full_backward_propagation(y_hat, y_train)\n",
    "#ffnn.update(grad_values)\n",
    "y_hat_test = ffnn.predict(X_test_T)\n",
    "\n",
    "# Rescaling the test predictions\n",
    "#y_hat_test = y_hat_test*(y_train_max - y_train_min) + y_train_min\n",
    "\n",
    "#print(y_hat_test)\n",
    "print(\"The testing MSE is : \" + str(ffnn.compute_mse(y_test, y_hat_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
