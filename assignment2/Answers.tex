\documentclass{article}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}

\title{Assignment 2: Naive Bayes and Text Classification}

\author{Benedikt Riegel, 3568633
    \and Fozan Gill, 3437081
    \and Srinivas Kumar R, 3513675}


\begin{document}
	\maketitle
	\section{Task 1: Simple Bayes (20 Points)}
	\subsection{Task (10 Points)}
	Define:
	\begin{enumerate}[label=-]
		\item $P(\text{Box x}) := \text{probability that Box x is chosen})$
		\item $P(\text{Apple}) := \text{probability that an apple is chosen}$
		\item $P(\text{Orange}) := \text{probability that an orange is chosen}$
	\end{enumerate}
	Given: $P(\text{Box 1}) = P(\text{Box 2})$
	\begin{center}
		\begin{tabular}{ |c|c|c| }
			\hline
			& Apples & Oranges \\
			\hline
			Box 1 & 4 & 10 \\
			\hline
			Box 2 & 6 & 8 \\
			\hline
		\end{tabular}
	\end{center}
	We assume $P(\text{Box 1}) + P(\text{Box 2}) = 1$.\\
	$\implies P(\text{Apple}\mid\text{Box 1}) = 4/14 = 2/7$, $P(\text{Orange}\mid\text{Box 1}) = 10/14 = 5/7$,\\
	$P(\text{Apple}\mid\text{Box 2}) = 6/14 = 3/7$ and $P(\text{Orange}\mid\text{Box 2}) = 8/14 = 4/7$.\\
	$P(\text{Box 1}) = P(\text{Box 2}) = 1/2$.\\\\
	What is the probability of choosing an apple?\\
	\begin{eqnarray*}
		P(\text{Apple}) &=& P(\text{Apple, Box 1}) + P(\text{Apple, Box 2})\\
		&=& P(\text{Box 1}) \cdot P(\text{Apple}\mid\text{Box 1}) + P(\text{Box 2}) \cdot P(\text{Apple}\mid\text{Box 2})\\
		&=& \frac{1}{2} \cdot \frac{2}{7} + \frac{1}{2} \cdot \frac{3}{7}\\
		&=& \frac{5}{14}
	\end{eqnarray*}
	If an apple is chosen, what is the probability that it came from box 1?\\
	\begin{eqnarray*}
		P(\text{Box 1} \mid \text{Apple}) &=& \frac{P(\text{Box 1, Apple})}{P(\text{Apple})}\\
		&=& \frac{P(\text{Apple, Box 1})}{P(\text{Apple})}\\
		&=& \frac{\frac{1}{2} \cdot \frac{2}{7}}{\frac{5}{14}}\\
		&=& \frac{1 \cdot 2 \cdot 14}{2 \cdot 7 \cdot 5}\\
		&=& \frac{28}{70}\\
		&=& \frac{2}{5}
	\end{eqnarray*}

	\subsection{Task (10 Points)}
	Given: Given are two M\&M bags from 1994 and 1996 and the probabilities of finding a specific colour in the two different bags. The probabilities are as follows:
	\begin{center}
		\begin{tabular}{ |c|c|c|c| }
			\hline
			& Yellow & Green & Other \\
			\hline
			1994 & 0.3 & 0.2 & 0.5 \\
			\hline
			1996 & 0.16 & 0.24 & 0.6 \\
			\hline
		\end{tabular}
	\end{center}
	Now scenario $A := $"one M\&M of each bag is taken out, one is green and the other is yellow." occurs.\\
	What is the probability of scenario $B := $ "the yellow M\&M came from the 1994 bag"?
	So it's asked for the probability
	\begin{equation*}
		P(B \mid A) = \frac{P(B, A)}{P(A)}
	\end{equation*}
	First take a look at $P(B, A)$, notice that $A$ and $B$ implies that not only is the 1994 M\&M yellow, but the 1996 M\&M is green.
	\begin{eqnarray*}
		P(B, A) &=& P(\text{1994 Bag was picked}) * P(\text{yellow M\&M})\\
		&&+ P(\text{1996 Bag was picked}) * P(\text{green M\&M})\\
		&=& 0.5 * 0.3 + 0.5 * 0.24\\
		&=& 0.27
	\end{eqnarray*}
	Notice that because of statement $A$ we have to take one out of each bag, so we have to treat the probabilities for each bag to be $0.5$.
	Now $P(A)$ can also be computed by summing up all the allowed combinations of M\&M that fulfil scenario $A$.\\
	\begin{enumerate}[label=A.\arabic*]
		\item 1994 M\&M is yellow $\implies$ 1996 M\&M is green.
		\item 1996 M\&M is green $\implies$ 1996 M\&M is yellow.
	\end{enumerate}
	This means there are only scenario $A.1$ and $A.2$ that fulfil $A$.
	\begin{eqnarray*}
		\left[A.1 \Leftrightarrow B \text{ and } A\right] \implies \left[P(A.1) = P(B, A) = 0.27\right]
	\end{eqnarray*}
	Analogously $P(A.2)$ can be computed to be
	\begin{eqnarray*}
		P(A.2) &=& 0.5 * 0.2 + 0.5 * 0.16\\
		&=& 0.18
	\end{eqnarray*}
	resulting in
	\begin{eqnarray*}
		P(A) &=& P(A.1) + P(A.2)\\
		&=& 0.27 + 0.18\\
		&=& 0.45
	\end{eqnarray*}
	Therefore the answer to our previous question is
	\begin{eqnarray*}
		P(B \mid A) &=& \frac{P(B, A)}{P(A)}\\
		&=& \frac{0.27}{0.45}\\
		&=& 0.6
	\end{eqnarray*}
\section{Task 2: Fake News Classification with Naive Bayes (20 Points)}
	This task was answered in the jupyter-notebook.
\section{Task 3: kNN for Text Classification (30 Points)}
	\subsection{How do you represent the text?}
	We could add one dimension for each term, which represents the frequency of this specific term. E.g.
	\begin{enumerate}[label=-]
		\item point $[0, 0]$ represents a text with $0$ 'hi's and $0$ 'bye's (empty text []).
		\item point $[2, 1]$ represents a text with $2$ 'hi's and $1$ 'bye' (['hi', 'hi', 'bye'], ['hi', 'bye', 'hi'], ['bye', 'hi', 'hi']).
	\end{enumerate}
	We should use a min-max scaling on this data.
	\subsection{What distance function do you use?}
	Specifically, four different distance functions, which are Euclidean distance, cosine, similarity measure, Minkowsky, correlation, and Chi square, are used in the kNN classifier.
        The distance function that is used by us as follows:
        Euclidean dist. (p,q) :
        \\
        $\dis d(\mathbf{p},\mathbf{q}) = \sqrt[2]{\sum_{i=1}^{n} (p_i - q_i)^{2}}$
        \\
        Advantage of Euclidean distance is that it measures the regular distance between two points in space. For this reason, it is widely used in the applications where the distance between data points are needed to be calculated to measure similarities.
    \subsection{What decision rule do you use?}
    We can use K-nearest neighbor for text classification. BKNN requires much less CPU time than KNN, without loss of classification performance.
\section{Task 4: Task 4: kNN in High-Dimensional Feature Spaces (30 Points)}
    \subsection{ Research and discuss why kNN might fail for high dimensional feature spaces.}
    With higher dimensions, the distance metric becomes meaningless in KNN as all points tend to be equidistant from each other as the dimensions tend to larger and larger values, so because of this the k set of points are equally distant to the test point as compared to other points hence the label assigned to the test points is faulty and the algorithm fails.
    \subsection{ Identify and explain one approach for solving or circumventing this problem.} As suggested in \cite{cnnknn} we can have an integrated model such as a CNN-KNN which first extracts only the relevant features of the data and reduces the dimensionality of the same, so this will ensure the KNN performs well on the lower dimension data.  Or as per this  \href{https://qr.ae/pGnZPO}{answer}, we can change the distance metric to something spherical based on cosine distance.


\begin{thebibliography}{9}
\bibitem{cnnknn}
A. D. Bhat, H. R. Acharya and S. H.R., "A Novel Solution to the Curse of Dimensionality in Using KNNs for Image Classification," 2019 2nd International Conference on Intelligent Autonomous Systems (ICoIAS), 2019, pp. 32-36, doi: 10.1109/ICoIAS.2019.00012.



\end{document}